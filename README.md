Offline RAG Assistant

A private, fully offline Retrieval-Augmented Generation system built with Ollama, FAISS, and Python, featuring a Streamlit UI, intelligent chunking, and complete local inference.
This project proves that powerful AI assistants can run entirely on your own machine without sending any data to the cloud.

â­ Features
ğŸ” Completely Offline

Local embeddings with nomic-embed-text

Local LLM generation with Llama 3.2

No API keys

No external network calls

Your data never leaves your device

ğŸ“„ Multi-Format Document Support

Supports:

PDF

Markdown

HTML

TXT

DOCX (optional add-on)

ğŸ” Retrieval Components

Chunking with overlap

Sentence-aware boundaries

Cosine similarity search

FAISS vector index

Adjustable thresholds

Transparent chunk metadata

ğŸ§  Answer Generation

Llama 3.2 for grounded responses

Strict â€œno hallucinationâ€ prompt rules

Cited answers with source tracking

Confidence scoring

ğŸ–¥ï¸ Streamlit UI

Chat interface

File uploader

Automatic re-indexing

Retrieval visualization

Clean, minimal layout

ğŸš€ Quick Start
1. Clone the repository
git clone https://github.com/saitejasrivilli/offline-rag-assistant.git
cd offline-rag-assistant

2. Install dependencies
pip install -r requirements.txt

3. Install Ollama

Download from: https://ollama.com/download

Verify:

ollama --version

4. Pull the models
ollama pull llama3.2
ollama pull nomic-embed-text

5. Run the core RAG engine
python3 rag.py

6. Run the UI
streamlit run app.py

ğŸ§© Project Structure
offline-rag-assistant/
â”‚â”€â”€ app.py                # Streamlit UI
â”‚â”€â”€ rag.py                # Core offline RAG engine
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ documents/            # User documents stored here
â”‚â”€â”€ vector_db/            # FAISS index + metadata
â”‚â”€â”€ README.md
â””â”€â”€ .gitignore
